# Linear Regression Optimizer Comparison

This project demonstrates and compares different optimization algorithms for linear regression, including:
- Gradient Descent
- Momentum
- Adam

## Features
- Synthetic data generation for linear regression
- Implementation of three optimizers
- Visualization of fitted lines and convergence curves
- Visualization of optimizer parameter paths on the error surface
- Comparison for both centered and non-centered data

## How to Run
1. Make sure you have Python 3 and the required packages:
   - numpy
   - matplotlib
2. Run the main script:
   ```bash
   python main.py
   ```
3. The script will display several plots comparing the optimizers and their convergence behavior.

## Files
- `main.py`: Main script for running experiments and visualizations
- `gradient_descent.py`: Gradient Descent implementation
- `momentum.py`: Momentum optimizer implementation
- `adam_optimizer.py`: Adam optimizer implementation

## Purpose
This project is intended for educational purposes to visualize and understand the behavior of different optimization algorithms in linear regression. 